{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "learning_rate_finder_tf2.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmKhG/learning_rate_finder_tf2/blob/master/learning_rate_finder_tf2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ6ncjWQnifL",
        "colab_type": "text"
      },
      "source": [
        "# Finding a good learning rate introduced by the paper: \"Cyclical Learning Rates for Training Neural Networks\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dn-6c02VmqiN",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p6rs9ynfW0N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ca0d16eb-250d-485b-ca15-4e0e11b75688"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "print('Train data: ', x_train.shape, y_train.shape)\n",
        "print('Test data', x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data:  (60000, 28, 28) (60000,)\n",
            "Test data (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3JHcAAG9hzv",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-BQrav4anTmj",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "def get_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    layers.Flatten(input_shape=(28,28)),\n",
        "    layers.Dense(units=28*28, activation='relu'),\n",
        "    layers.Dense(units=64, activation='relu'),\n",
        "    layers.Dense(units=10, activation='softmax')\n",
        "  ])\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-08), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5jutTqLoIC1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "d34129f4-ae7f-4bc4-9af0-33a2d6b283a5"
      },
      "source": [
        "model = get_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_13 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 784)               615440    \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 666,330\n",
            "Trainable params: 666,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7t3eEhpyUos",
        "colab_type": "text"
      },
      "source": [
        "### Learning rate finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc2tjVHjgDg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b12c4bbc-747a-4663-962e-7001721bc646"
      },
      "source": [
        "# need to know the number of batch per epoch\n",
        "batch_size=32\n",
        "num_batch_per_epoch=x_train.shape[0] / batch_size\n",
        "print(num_batch_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUH3J8Qeyd45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import callbacks\n",
        "import tensorflow.keras.backend as backend\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# credit goes to here:\n",
        "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#how-do-you-find-a-good-learning-rate\n",
        "\n",
        "class find_lr(callbacks.Callback):\n",
        "\n",
        "  def __init__(self, init_value=5e-04, \n",
        "               final_value=10.0, \n",
        "               beta=0.98, \n",
        "               # use the value above here\n",
        "               num_batch_per_epoch=1875.0):\n",
        "    #b/c we call on_batch_end and not on_batch_begin(b/c we won't have loss)\n",
        "    self.batch_num = 1\n",
        "    self.avg_loss = 0.0\n",
        "    self.beta = beta\n",
        "    self.best_loss = 0.0\n",
        "    self.losses = []\n",
        "    self.lr_val = init_value\n",
        "    self.final_lr_val = final_value\n",
        "    self.num_batch_per_epoch = num_batch_per_epoch\n",
        "    self.mult = (self.final_lr_val / self.lr_val) ** (1.0 / self.num_batch_per_epoch)\n",
        "    self.log_lrs = []\n",
        "    self.lrs = []\n",
        "\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    # tensorflow will train till end of epoch, but we don't need to update here\n",
        "    if self.model.stop_training:\n",
        "      return\n",
        "\n",
        "    self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * logs['loss']\n",
        "    smoothed_loss = self.avg_loss / (1.0 - self.beta ** self.batch_num)\n",
        "    if self.batch_num > 1 and smoothed_loss > 4 * self.best_loss:\n",
        "      print('\\nLoss is too big now, cancelling training.')\n",
        "      self.model.stop_training = True\n",
        "      return\n",
        "\n",
        "    if smoothed_loss < self.best_loss or self.batch_num == 1:\n",
        "      self.best_loss = smoothed_loss\n",
        "\n",
        "    self.losses.append(smoothed_loss)\n",
        "    self.log_lrs.append(np.math.log10(self.lr_val))\n",
        "\n",
        "    # new lr\n",
        "    self.lr_val *= self.mult\n",
        "    backend.set_value(self.model.optimizer.learning_rate, self.lr_val)\n",
        "    print('\\nbatch_num: {}, lr: {}, loss: {:3.2f}, smoothed_loss: {:3.2f}, best_loss: {:3.2f}'.format(self.batch_num, \n",
        "                                                                                       self.model.optimizer.lr.numpy(), \n",
        "                                                                                       logs['loss'],\n",
        "                                                                                       smoothed_loss,\n",
        "                                                                                       self.best_loss))\n",
        "\n",
        "    self.batch_num += 1\n",
        "\n",
        "  def plot_lr(self):\n",
        "    \"\"\"\n",
        "    Plots the learning rate\n",
        "    \"\"\"\n",
        "    plt.plot(self.log_lrs[10:-5], self.losses[10:-5])\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('learning_rate(log scale)')\n",
        "    plt.title('Loss vs LR')\n",
        "\n",
        "lr_finder = find_lr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD7ywn5C2TyH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e39b5bd1-8ec8-4a82-9c8e-ce818a11adbb"
      },
      "source": [
        "model = get_model()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=1,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[lr_finder])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_14 (Flatten)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 784)               615440    \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 64)                50240     \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 666,330\n",
            "Trainable params: 666,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "batch_num: 1, lr: 0.0005026478902436793, loss: 123.23, smoothed_loss: 123.23, best_loss: 123.23\n",
            "   1/1875 [..............................] - ETA: 0s - loss: 123.2293 - accuracy: 0.0312\n",
            "batch_num: 2, lr: 0.000505309842992574, loss: 125.76, smoothed_loss: 124.51, best_loss: 123.23\n",
            "\n",
            "batch_num: 3, lr: 0.0005079858819954097, loss: 128.05, smoothed_loss: 125.71, best_loss: 123.23\n",
            "\n",
            "batch_num: 4, lr: 0.0005106761236675084, loss: 166.29, smoothed_loss: 136.17, best_loss: 123.23\n",
            "\n",
            "batch_num: 5, lr: 0.0005133805680088699, loss: 146.94, smoothed_loss: 138.41, best_loss: 123.23\n",
            "\n",
            "batch_num: 6, lr: 0.0005160993314348161, loss: 127.49, smoothed_loss: 136.50, best_loss: 123.23\n",
            "\n",
            "batch_num: 7, lr: 0.0005188325303606689, loss: 115.51, smoothed_loss: 133.31, best_loss: 123.23\n",
            "\n",
            "batch_num: 8, lr: 0.0005215801647864282, loss: 102.74, smoothed_loss: 129.22, best_loss: 123.23\n",
            "\n",
            "batch_num: 9, lr: 0.0005243423511274159, loss: 92.35, smoothed_loss: 124.78, best_loss: 123.23\n",
            "\n",
            "batch_num: 10, lr: 0.0005271192057989538, loss: 83.77, smoothed_loss: 120.30, best_loss: 120.30\n",
            "\n",
            "batch_num: 11, lr: 0.0005299107288010418, loss: 76.59, smoothed_loss: 115.91, best_loss: 115.91\n",
            "\n",
            "batch_num: 12, lr: 0.0005327170365490019, loss: 70.56, smoothed_loss: 111.70, best_loss: 111.70\n",
            "\n",
            "batch_num: 13, lr: 0.0005355382454581559, loss: 65.38, smoothed_loss: 107.69, best_loss: 107.69\n",
            "\n",
            "batch_num: 14, lr: 0.0005383743555285037, loss: 61.04, smoothed_loss: 103.90, best_loss: 103.90\n",
            "  14/1875 [..............................] - ETA: 6s - loss: 61.0442 - accuracy: 0.2009 \n",
            "batch_num: 15, lr: 0.0005412254831753671, loss: 57.28, smoothed_loss: 100.33, best_loss: 100.33\n",
            "\n",
            "batch_num: 16, lr: 0.0005440917448140681, loss: 53.90, smoothed_loss: 96.97, best_loss: 96.97\n",
            "\n",
            "batch_num: 17, lr: 0.0005469731404446065, loss: 50.96, smoothed_loss: 93.81, best_loss: 93.81\n",
            "\n",
            "batch_num: 18, lr: 0.0005498698446899652, loss: 48.25, smoothed_loss: 90.82, best_loss: 90.82\n",
            "\n",
            "batch_num: 19, lr: 0.0005527818575501442, loss: 45.86, smoothed_loss: 88.00, best_loss: 88.00\n",
            "\n",
            "batch_num: 20, lr: 0.0005557092954404652, loss: 43.71, smoothed_loss: 85.33, best_loss: 85.33\n",
            "\n",
            "batch_num: 21, lr: 0.0005586522165685892, loss: 41.79, smoothed_loss: 82.81, best_loss: 82.81\n",
            "\n",
            "batch_num: 22, lr: 0.000561610737349838, loss: 39.97, smoothed_loss: 80.43, best_loss: 80.43\n",
            "\n",
            "batch_num: 23, lr: 0.0005645849741995335, loss: 38.33, smoothed_loss: 78.16, best_loss: 78.16\n",
            "\n",
            "batch_num: 24, lr: 0.0005675749271176755, loss: 36.82, smoothed_loss: 76.01, best_loss: 76.01\n",
            "\n",
            "batch_num: 25, lr: 0.0005705807125195861, loss: 35.43, smoothed_loss: 73.96, best_loss: 73.96\n",
            "\n",
            "batch_num: 26, lr: 0.000573602388612926, loss: 34.16, smoothed_loss: 72.01, best_loss: 72.01\n",
            "\n",
            "batch_num: 27, lr: 0.0005766400718130171, loss: 32.97, smoothed_loss: 70.16, best_loss: 70.16\n",
            "  27/1875 [..............................] - ETA: 7s - loss: 32.9688 - accuracy: 0.2315\n",
            "batch_num: 28, lr: 0.0005796938785351813, loss: 31.87, smoothed_loss: 68.38, best_loss: 68.38\n",
            "\n",
            "batch_num: 29, lr: 0.0005827638669870794, loss: 30.87, smoothed_loss: 66.69, best_loss: 66.69\n",
            "\n",
            "batch_num: 30, lr: 0.0005858500953763723, loss: 29.95, smoothed_loss: 65.08, best_loss: 65.08\n",
            "\n",
            "batch_num: 31, lr: 0.0005889526219107211, loss: 29.05, smoothed_loss: 63.53, best_loss: 63.53\n",
            "\n",
            "batch_num: 32, lr: 0.0005920716212131083, loss: 28.21, smoothed_loss: 62.04, best_loss: 62.04\n",
            "\n",
            "batch_num: 33, lr: 0.000595207151491195, loss: 27.42, smoothed_loss: 60.62, best_loss: 60.62\n",
            "\n",
            "batch_num: 34, lr: 0.000598359270952642, loss: 26.69, smoothed_loss: 59.25, best_loss: 59.25\n",
            "\n",
            "batch_num: 35, lr: 0.0006015280960127711, loss: 25.97, smoothed_loss: 57.94, best_loss: 57.94\n",
            "\n",
            "batch_num: 36, lr: 0.0006047136848792434, loss: 25.31, smoothed_loss: 56.68, best_loss: 56.68\n",
            "\n",
            "batch_num: 37, lr: 0.0006079160957597196, loss: 24.70, smoothed_loss: 55.46, best_loss: 55.46\n",
            "\n",
            "batch_num: 38, lr: 0.0006111355614848435, loss: 24.11, smoothed_loss: 54.29, best_loss: 54.29\n",
            "\n",
            "batch_num: 39, lr: 0.0006143720238469541, loss: 23.54, smoothed_loss: 53.17, best_loss: 53.17\n",
            "  39/1875 [..............................] - ETA: 7s - loss: 23.5380 - accuracy: 0.2532\n",
            "batch_num: 40, lr: 0.0006176256574690342, loss: 23.01, smoothed_loss: 52.08, best_loss: 52.08\n",
            "\n",
            "batch_num: 41, lr: 0.0006208964623510838, loss: 22.51, smoothed_loss: 51.03, best_loss: 51.03\n",
            "\n",
            "batch_num: 42, lr: 0.0006241846131160855, loss: 22.02, smoothed_loss: 50.01, best_loss: 50.01\n",
            "\n",
            "batch_num: 43, lr: 0.0006274902261793613, loss: 21.55, smoothed_loss: 49.03, best_loss: 49.03\n",
            "\n",
            "batch_num: 44, lr: 0.0006308133015409112, loss: 21.10, smoothed_loss: 48.08, best_loss: 48.08\n",
            "\n",
            "batch_num: 45, lr: 0.0006341539556160569, loss: 20.68, smoothed_loss: 47.17, best_loss: 47.17\n",
            "\n",
            "batch_num: 46, lr: 0.0006375123630277812, loss: 20.30, smoothed_loss: 46.28, best_loss: 46.28\n",
            "\n",
            "batch_num: 47, lr: 0.0006408885237760842, loss: 19.92, smoothed_loss: 45.42, best_loss: 45.42\n",
            "\n",
            "batch_num: 48, lr: 0.0006442825542762876, loss: 19.55, smoothed_loss: 44.59, best_loss: 44.59\n",
            "\n",
            "batch_num: 49, lr: 0.0006476945709437132, loss: 19.19, smoothed_loss: 43.78, best_loss: 43.78\n",
            "\n",
            "batch_num: 50, lr: 0.000651124631986022, loss: 18.85, smoothed_loss: 42.99, best_loss: 42.99\n",
            "\n",
            "batch_num: 51, lr: 0.0006545729120261967, loss: 18.53, smoothed_loss: 42.23, best_loss: 42.23\n",
            "  51/1875 [..............................] - ETA: 7s - loss: 18.5290 - accuracy: 0.2592\n",
            "batch_num: 52, lr: 0.0006580394110642374, loss: 18.23, smoothed_loss: 41.49, best_loss: 41.49\n",
            "\n",
            "batch_num: 53, lr: 0.0006615243037231266, loss: 17.92, smoothed_loss: 40.78, best_loss: 40.78\n",
            "\n",
            "batch_num: 54, lr: 0.0006650275900028646, loss: 17.64, smoothed_loss: 40.08, best_loss: 40.08\n",
            "\n",
            "batch_num: 55, lr: 0.0006685494445264339, loss: 17.36, smoothed_loss: 39.40, best_loss: 39.40\n",
            "\n",
            "batch_num: 56, lr: 0.0006720899837091565, loss: 17.08, smoothed_loss: 38.74, best_loss: 38.74\n",
            "\n",
            "batch_num: 57, lr: 0.0006756492657586932, loss: 16.83, smoothed_loss: 38.10, best_loss: 38.10\n",
            "\n",
            "batch_num: 58, lr: 0.0006792274070903659, loss: 16.56, smoothed_loss: 37.48, best_loss: 37.48\n",
            "\n",
            "batch_num: 59, lr: 0.0006828244659118354, loss: 16.31, smoothed_loss: 36.87, best_loss: 36.87\n",
            "\n",
            "batch_num: 60, lr: 0.0006864406168460846, loss: 16.07, smoothed_loss: 36.28, best_loss: 36.28\n",
            "\n",
            "batch_num: 61, lr: 0.0006900758598931134, loss: 15.85, smoothed_loss: 35.70, best_loss: 35.70\n",
            "\n",
            "batch_num: 62, lr: 0.0006937304278835654, loss: 15.62, smoothed_loss: 35.14, best_loss: 35.14\n",
            "\n",
            "batch_num: 63, lr: 0.0006974042626097798, loss: 15.41, smoothed_loss: 34.59, best_loss: 34.59\n",
            "\n",
            "batch_num: 64, lr: 0.0007010975969024003, loss: 15.20, smoothed_loss: 34.06, best_loss: 34.06\n",
            "  64/1875 [>.............................] - ETA: 7s - loss: 15.2008 - accuracy: 0.2686\n",
            "batch_num: 65, lr: 0.0007048105471767485, loss: 15.00, smoothed_loss: 33.54, best_loss: 33.54\n",
            "\n",
            "batch_num: 66, lr: 0.0007085430552251637, loss: 14.80, smoothed_loss: 33.03, best_loss: 33.03\n",
            "\n",
            "batch_num: 67, lr: 0.0007122954120859504, loss: 14.60, smoothed_loss: 32.53, best_loss: 32.53\n",
            "\n",
            "batch_num: 68, lr: 0.0007160676177591085, loss: 14.42, smoothed_loss: 32.04, best_loss: 32.04\n",
            "\n",
            "batch_num: 69, lr: 0.00071985978865996, loss: 14.23, smoothed_loss: 31.57, best_loss: 31.57\n",
            "\n",
            "batch_num: 70, lr: 0.0007236720412038267, loss: 14.06, smoothed_loss: 31.11, best_loss: 31.11\n",
            "\n",
            "batch_num: 71, lr: 0.0007275044918060303, loss: 13.88, smoothed_loss: 30.66, best_loss: 30.66\n",
            "\n",
            "batch_num: 72, lr: 0.0007313571986742318, loss: 13.71, smoothed_loss: 30.21, best_loss: 30.21\n",
            "\n",
            "batch_num: 73, lr: 0.0007352303946390748, loss: 13.55, smoothed_loss: 29.78, best_loss: 29.78\n",
            "\n",
            "batch_num: 74, lr: 0.0007391240214928985, loss: 13.39, smoothed_loss: 29.36, best_loss: 29.36\n",
            "\n",
            "batch_num: 75, lr: 0.0007430383120663464, loss: 13.24, smoothed_loss: 28.95, best_loss: 28.95\n",
            "\n",
            "batch_num: 76, lr: 0.0007469733245670795, loss: 13.10, smoothed_loss: 28.54, best_loss: 28.54\n",
            "\n",
            "batch_num: 77, lr: 0.0007509291754104197, loss: 12.96, smoothed_loss: 28.15, best_loss: 28.15\n",
            "\n",
            "batch_num: 78, lr: 0.0007549059810116887, loss: 12.81, smoothed_loss: 27.76, best_loss: 27.76\n",
            "  78/1875 [>.............................] - ETA: 7s - loss: 12.8110 - accuracy: 0.2841\n",
            "batch_num: 79, lr: 0.0007589037995785475, loss: 12.67, smoothed_loss: 27.38, best_loss: 27.38\n",
            "\n",
            "batch_num: 80, lr: 0.0007629228639416397, loss: 12.54, smoothed_loss: 27.01, best_loss: 27.01\n",
            "\n",
            "batch_num: 81, lr: 0.0007669631741009653, loss: 12.41, smoothed_loss: 26.65, best_loss: 26.65\n",
            "\n",
            "batch_num: 82, lr: 0.0007710248464718461, loss: 12.29, smoothed_loss: 26.29, best_loss: 26.29\n",
            "\n",
            "batch_num: 83, lr: 0.0007751080556772649, loss: 12.16, smoothed_loss: 25.95, best_loss: 25.95\n",
            "\n",
            "batch_num: 84, lr: 0.0007792129181325436, loss: 12.03, smoothed_loss: 25.61, best_loss: 25.61\n",
            "\n",
            "batch_num: 85, lr: 0.0007833394920453429, loss: 11.92, smoothed_loss: 25.27, best_loss: 25.27\n",
            "\n",
            "batch_num: 86, lr: 0.0007874879520386457, loss: 11.80, smoothed_loss: 24.94, best_loss: 24.94\n",
            "\n",
            "batch_num: 87, lr: 0.0007916583563201129, loss: 11.69, smoothed_loss: 24.62, best_loss: 24.62\n",
            "\n",
            "batch_num: 88, lr: 0.0007958508213050663, loss: 11.57, smoothed_loss: 24.31, best_loss: 24.31\n",
            "\n",
            "batch_num: 89, lr: 0.0008000655216164887, loss: 11.46, smoothed_loss: 24.00, best_loss: 24.00\n",
            "\n",
            "batch_num: 90, lr: 0.0008043025736697018, loss: 11.35, smoothed_loss: 23.70, best_loss: 23.70\n",
            "\n",
            "batch_num: 91, lr: 0.0008085619774647057, loss: 11.25, smoothed_loss: 23.40, best_loss: 23.40\n",
            "  91/1875 [>.............................] - ETA: 7s - loss: 11.2500 - accuracy: 0.2830\n",
            "batch_num: 92, lr: 0.0008128440240398049, loss: 11.15, smoothed_loss: 23.11, best_loss: 23.11\n",
            "\n",
            "batch_num: 93, lr: 0.0008171487133949995, loss: 11.05, smoothed_loss: 22.83, best_loss: 22.83\n",
            "\n",
            "batch_num: 94, lr: 0.0008214761619456112, loss: 10.95, smoothed_loss: 22.55, best_loss: 22.55\n",
            "\n",
            "batch_num: 95, lr: 0.0008258266025222838, loss: 10.85, smoothed_loss: 22.28, best_loss: 22.28\n",
            "\n",
            "batch_num: 96, lr: 0.0008302000351250172, loss: 10.76, smoothed_loss: 22.01, best_loss: 22.01\n",
            "\n",
            "batch_num: 97, lr: 0.0008345966343767941, loss: 10.66, smoothed_loss: 21.74, best_loss: 21.74\n",
            "\n",
            "batch_num: 98, lr: 0.0008390165166929364, loss: 10.58, smoothed_loss: 21.48, best_loss: 21.48\n",
            "\n",
            "batch_num: 99, lr: 0.000843459798488766, loss: 10.49, smoothed_loss: 21.23, best_loss: 21.23\n",
            "\n",
            "batch_num: 100, lr: 0.0008479265961796045, loss: 10.40, smoothed_loss: 20.98, best_loss: 20.98\n",
            "\n",
            "batch_num: 101, lr: 0.0008524170843884349, loss: 10.31, smoothed_loss: 20.73, best_loss: 20.73\n",
            "\n",
            "batch_num: 102, lr: 0.0008569313795305789, loss: 10.23, smoothed_loss: 20.49, best_loss: 20.49\n",
            "\n",
            "batch_num: 103, lr: 0.0008614695398136973, loss: 10.15, smoothed_loss: 20.26, best_loss: 20.26\n",
            "\n",
            "batch_num: 104, lr: 0.0008660317398607731, loss: 10.07, smoothed_loss: 20.02, best_loss: 20.02\n",
            "\n",
            "batch_num: 105, lr: 0.0008706180960871279, loss: 10.00, smoothed_loss: 19.80, best_loss: 19.80\n",
            " 105/1875 [>.............................] - ETA: 6s - loss: 9.9977 - accuracy: 0.2896 \n",
            "batch_num: 106, lr: 0.0008752287249080837, loss: 9.92, smoothed_loss: 19.57, best_loss: 19.57\n",
            "\n",
            "batch_num: 107, lr: 0.0008798638009466231, loss: 9.84, smoothed_loss: 19.35, best_loss: 19.35\n",
            "\n",
            "batch_num: 108, lr: 0.0008845233824104071, loss: 9.77, smoothed_loss: 19.14, best_loss: 19.14\n",
            "\n",
            "batch_num: 109, lr: 0.0008892077021300793, loss: 9.70, smoothed_loss: 18.92, best_loss: 18.92\n",
            "\n",
            "batch_num: 110, lr: 0.0008939168183133006, loss: 9.62, smoothed_loss: 18.72, best_loss: 18.72\n",
            "\n",
            "batch_num: 111, lr: 0.0008986508473753929, loss: 9.55, smoothed_loss: 18.51, best_loss: 18.51\n",
            "\n",
            "batch_num: 112, lr: 0.0009034099639393389, loss: 9.48, smoothed_loss: 18.31, best_loss: 18.31\n",
            "\n",
            "batch_num: 113, lr: 0.0009081942262127995, loss: 9.41, smoothed_loss: 18.11, best_loss: 18.11\n",
            "\n",
            "batch_num: 114, lr: 0.0009130038670264184, loss: 9.34, smoothed_loss: 17.92, best_loss: 17.92\n",
            "\n",
            "batch_num: 115, lr: 0.0009178390027955174, loss: 9.28, smoothed_loss: 17.72, best_loss: 17.72\n",
            "\n",
            "batch_num: 116, lr: 0.0009226997499354184, loss: 9.21, smoothed_loss: 17.54, best_loss: 17.54\n",
            "\n",
            "batch_num: 117, lr: 0.0009275861666537821, loss: 9.15, smoothed_loss: 17.35, best_loss: 17.35\n",
            "\n",
            "batch_num: 118, lr: 0.0009324985439889133, loss: 9.09, smoothed_loss: 17.17, best_loss: 17.17\n",
            " 118/1875 [>.............................] - ETA: 6s - loss: 9.0883 - accuracy: 0.2998\n",
            "batch_num: 119, lr: 0.0009374368819408119, loss: 9.03, smoothed_loss: 16.99, best_loss: 16.99\n",
            "\n",
            "batch_num: 120, lr: 0.0009424014133401215, loss: 8.97, smoothed_loss: 16.81, best_loss: 16.81\n",
            "\n",
            "batch_num: 121, lr: 0.0009473921963945031, loss: 8.91, smoothed_loss: 16.64, best_loss: 16.64\n",
            "\n",
            "batch_num: 122, lr: 0.0009524094639346004, loss: 8.84, smoothed_loss: 16.47, best_loss: 16.47\n",
            "\n",
            "batch_num: 123, lr: 0.0009574532159604132, loss: 8.80, smoothed_loss: 16.30, best_loss: 16.30\n",
            "\n",
            "batch_num: 124, lr: 0.0009625237435102463, loss: 8.75, smoothed_loss: 16.14, best_loss: 16.14\n",
            "\n",
            "batch_num: 125, lr: 0.0009676211047917604, loss: 8.69, smoothed_loss: 15.98, best_loss: 15.98\n",
            "\n",
            "batch_num: 126, lr: 0.0009727454744279385, loss: 8.63, smoothed_loss: 15.82, best_loss: 15.82\n",
            "\n",
            "batch_num: 127, lr: 0.0009778969688341022, loss: 8.58, smoothed_loss: 15.66, best_loss: 15.66\n",
            "\n",
            "batch_num: 128, lr: 0.0009830757044255733, loss: 8.53, smoothed_loss: 15.51, best_loss: 15.51\n",
            "\n",
            "batch_num: 129, lr: 0.0009882819140329957, loss: 8.48, smoothed_loss: 15.35, best_loss: 15.35\n",
            "\n",
            "batch_num: 130, lr: 0.000993515714071691, loss: 8.42, smoothed_loss: 15.21, best_loss: 15.21\n",
            "\n",
            "batch_num: 131, lr: 0.0009987772209569812, loss: 8.38, smoothed_loss: 15.06, best_loss: 15.06\n",
            "\n",
            "batch_num: 132, lr: 0.001004066551104188, loss: 8.33, smoothed_loss: 14.91, best_loss: 14.91\n",
            " 132/1875 [=>............................] - ETA: 6s - loss: 8.3268 - accuracy: 0.3113\n",
            "batch_num: 133, lr: 0.001009383937343955, loss: 8.28, smoothed_loss: 14.77, best_loss: 14.77\n",
            "\n",
            "batch_num: 134, lr: 0.0010147294960916042, loss: 8.23, smoothed_loss: 14.63, best_loss: 14.63\n",
            "\n",
            "batch_num: 135, lr: 0.0010201033437624574, loss: 8.18, smoothed_loss: 14.49, best_loss: 14.49\n",
            "\n",
            "batch_num: 136, lr: 0.0010255055967718363, loss: 8.14, smoothed_loss: 14.36, best_loss: 14.36\n",
            "\n",
            "batch_num: 137, lr: 0.0010309364879503846, loss: 8.10, smoothed_loss: 14.22, best_loss: 14.22\n",
            "\n",
            "batch_num: 138, lr: 0.001036396250128746, loss: 8.05, smoothed_loss: 14.09, best_loss: 14.09\n",
            "\n",
            "batch_num: 139, lr: 0.0010418847668915987, loss: 8.01, smoothed_loss: 13.96, best_loss: 13.96\n",
            "\n",
            "batch_num: 140, lr: 0.001047402387484908, loss: 7.97, smoothed_loss: 13.84, best_loss: 13.84\n",
            "\n",
            "batch_num: 141, lr: 0.001052949344739318, loss: 7.92, smoothed_loss: 13.71, best_loss: 13.71\n",
            "\n",
            "batch_num: 142, lr: 0.0010585255222395062, loss: 7.88, smoothed_loss: 13.59, best_loss: 13.59\n",
            "\n",
            "batch_num: 143, lr: 0.0010641312692314386, loss: 7.84, smoothed_loss: 13.46, best_loss: 13.46\n",
            "\n",
            "batch_num: 144, lr: 0.0010697668185457587, loss: 7.79, smoothed_loss: 13.34, best_loss: 13.34\n",
            "\n",
            "batch_num: 145, lr: 0.0010754320537671447, loss: 7.75, smoothed_loss: 13.23, best_loss: 13.23\n",
            " 145/1875 [=>............................] - ETA: 6s - loss: 7.7519 - accuracy: 0.3209\n",
            "batch_num: 146, lr: 0.0010811274405568838, loss: 7.71, smoothed_loss: 13.11, best_loss: 13.11\n",
            "\n",
            "batch_num: 147, lr: 0.0010868528624996543, loss: 7.67, smoothed_loss: 13.00, best_loss: 13.00\n",
            "\n",
            "batch_num: 148, lr: 0.0010926086688414216, loss: 7.63, smoothed_loss: 12.88, best_loss: 12.88\n",
            "\n",
            "batch_num: 149, lr: 0.0010983949759975076, loss: 7.59, smoothed_loss: 12.77, best_loss: 12.77\n",
            "\n",
            "batch_num: 150, lr: 0.001104211900383234, loss: 7.56, smoothed_loss: 12.66, best_loss: 12.66\n",
            "\n",
            "batch_num: 151, lr: 0.0011100595584139228, loss: 7.52, smoothed_loss: 12.55, best_loss: 12.55\n",
            "\n",
            "batch_num: 152, lr: 0.0011159382993355393, loss: 7.48, smoothed_loss: 12.45, best_loss: 12.45\n",
            "\n",
            "batch_num: 153, lr: 0.0011218481231480837, loss: 7.45, smoothed_loss: 12.34, best_loss: 12.34\n",
            "\n",
            "batch_num: 154, lr: 0.0011277892626821995, loss: 7.41, smoothed_loss: 12.24, best_loss: 12.24\n",
            "\n",
            "batch_num: 155, lr: 0.0011337618343532085, loss: 7.38, smoothed_loss: 12.14, best_loss: 12.14\n",
            "\n",
            "batch_num: 156, lr: 0.0011397659545764327, loss: 7.34, smoothed_loss: 12.04, best_loss: 12.04\n",
            "\n",
            "batch_num: 157, lr: 0.0011458019725978374, loss: 7.31, smoothed_loss: 11.94, best_loss: 11.94\n",
            " 157/1875 [=>............................] - ETA: 6s - loss: 7.3073 - accuracy: 0.3240\n",
            "batch_num: 158, lr: 0.0011518700048327446, loss: 7.27, smoothed_loss: 11.84, best_loss: 11.84\n",
            "\n",
            "batch_num: 159, lr: 0.0011579700512811542, loss: 7.24, smoothed_loss: 11.74, best_loss: 11.74\n",
            "\n",
            "batch_num: 160, lr: 0.0011641025776043534, loss: 7.20, smoothed_loss: 11.65, best_loss: 11.65\n",
            "\n",
            "batch_num: 161, lr: 0.0011702674673870206, loss: 7.17, smoothed_loss: 11.56, best_loss: 11.56\n",
            "\n",
            "batch_num: 162, lr: 0.0011764649534597993, loss: 7.14, smoothed_loss: 11.47, best_loss: 11.47\n",
            "\n",
            "batch_num: 163, lr: 0.0011826952686533332, loss: 7.10, smoothed_loss: 11.37, best_loss: 11.37\n",
            "\n",
            "batch_num: 164, lr: 0.001188958645798266, loss: 7.07, smoothed_loss: 11.29, best_loss: 11.29\n",
            "\n",
            "batch_num: 165, lr: 0.0011952552013099194, loss: 7.04, smoothed_loss: 11.20, best_loss: 11.20\n",
            "\n",
            "batch_num: 166, lr: 0.0012015850516036153, loss: 7.01, smoothed_loss: 11.11, best_loss: 11.11\n",
            " 166/1875 [=>............................] - ETA: 6s - loss: 7.0120 - accuracy: 0.3255\n",
            "batch_num: 167, lr: 0.0012079484295099974, loss: 6.98, smoothed_loss: 11.03, best_loss: 11.03\n",
            "\n",
            "batch_num: 168, lr: 0.0012143455678597093, loss: 6.95, smoothed_loss: 10.94, best_loss: 10.94\n",
            "\n",
            "batch_num: 169, lr: 0.0012207765830680728, loss: 6.92, smoothed_loss: 10.86, best_loss: 10.86\n",
            "\n",
            "batch_num: 170, lr: 0.0012272415915504098, loss: 6.89, smoothed_loss: 10.78, best_loss: 10.78\n",
            "\n",
            "batch_num: 171, lr: 0.001233740826137364, loss: 6.86, smoothed_loss: 10.69, best_loss: 10.69\n",
            "\n",
            "batch_num: 172, lr: 0.0012402745196595788, loss: 6.83, smoothed_loss: 10.62, best_loss: 10.62\n",
            "\n",
            "batch_num: 173, lr: 0.0012468427885323763, loss: 6.80, smoothed_loss: 10.54, best_loss: 10.54\n",
            "\n",
            "batch_num: 174, lr: 0.0012534458655864, loss: 6.78, smoothed_loss: 10.46, best_loss: 10.46\n",
            "\n",
            "batch_num: 175, lr: 0.0012600838672369719, loss: 6.75, smoothed_loss: 10.38, best_loss: 10.38\n",
            "\n",
            "batch_num: 176, lr: 0.0012667571427300572, loss: 6.72, smoothed_loss: 10.31, best_loss: 10.31\n",
            " 176/1875 [=>............................] - ETA: 6s - loss: 6.7224 - accuracy: 0.3287\n",
            "batch_num: 177, lr: 0.0012734656920656562, loss: 6.70, smoothed_loss: 10.23, best_loss: 10.23\n",
            "\n",
            "batch_num: 178, lr: 0.0012802097480744123, loss: 6.67, smoothed_loss: 10.16, best_loss: 10.16\n",
            "\n",
            "batch_num: 179, lr: 0.0012869895435869694, loss: 6.65, smoothed_loss: 10.09, best_loss: 10.09\n",
            "\n",
            "batch_num: 180, lr: 0.001293805195018649, loss: 6.62, smoothed_loss: 10.02, best_loss: 10.02\n",
            "\n",
            "batch_num: 181, lr: 0.0013006569352000952, loss: 6.59, smoothed_loss: 9.95, best_loss: 9.95\n",
            "\n",
            "batch_num: 182, lr: 0.0013075449969619513, loss: 6.56, smoothed_loss: 9.88, best_loss: 9.88\n",
            "\n",
            "batch_num: 183, lr: 0.001314469613134861, loss: 6.54, smoothed_loss: 9.81, best_loss: 9.81\n",
            "\n",
            "batch_num: 184, lr: 0.0013214307837188244, loss: 6.51, smoothed_loss: 9.74, best_loss: 9.74\n",
            "\n",
            "batch_num: 185, lr: 0.001328428857959807, loss: 6.49, smoothed_loss: 9.67, best_loss: 9.67\n",
            "\n",
            "batch_num: 186, lr: 0.0013354639522731304, loss: 6.46, smoothed_loss: 9.61, best_loss: 9.61\n",
            "\n",
            "batch_num: 187, lr: 0.0013425364159047604, loss: 6.44, smoothed_loss: 9.54, best_loss: 9.54\n",
            "\n",
            "batch_num: 188, lr: 0.0013496462488546968, loss: 6.41, smoothed_loss: 9.48, best_loss: 9.48\n",
            "\n",
            "batch_num: 189, lr: 0.0013567936839535832, loss: 6.39, smoothed_loss: 9.42, best_loss: 9.42\n",
            " 189/1875 [==>...........................] - ETA: 6s - loss: 6.3921 - accuracy: 0.3347\n",
            "batch_num: 190, lr: 0.0013639790704473853, loss: 6.37, smoothed_loss: 9.35, best_loss: 9.35\n",
            "\n",
            "batch_num: 191, lr: 0.0013712025247514248, loss: 6.35, smoothed_loss: 9.29, best_loss: 9.29\n",
            "\n",
            "batch_num: 192, lr: 0.0013784641632810235, loss: 6.32, smoothed_loss: 9.23, best_loss: 9.23\n",
            "\n",
            "batch_num: 193, lr: 0.001385764218866825, loss: 6.30, smoothed_loss: 9.17, best_loss: 9.17\n",
            "\n",
            "batch_num: 194, lr: 0.001393103040754795, loss: 6.27, smoothed_loss: 9.11, best_loss: 9.11\n",
            "\n",
            "batch_num: 195, lr: 0.0014004806289449334, loss: 6.25, smoothed_loss: 9.05, best_loss: 9.05\n",
            "\n",
            "batch_num: 196, lr: 0.0014078974490985274, loss: 6.23, smoothed_loss: 9.00, best_loss: 9.00\n",
            "\n",
            "batch_num: 197, lr: 0.0014153533848002553, loss: 6.20, smoothed_loss: 8.94, best_loss: 8.94\n",
            "\n",
            "batch_num: 198, lr: 0.0014228489017114043, loss: 6.18, smoothed_loss: 8.88, best_loss: 8.88\n",
            "\n",
            "batch_num: 199, lr: 0.0014303839998319745, loss: 6.16, smoothed_loss: 8.83, best_loss: 8.83\n",
            "\n",
            "batch_num: 200, lr: 0.0014379591448232532, loss: 6.13, smoothed_loss: 8.77, best_loss: 8.77\n",
            "\n",
            "batch_num: 201, lr: 0.0014455743366852403, loss: 6.11, smoothed_loss: 8.72, best_loss: 8.72\n",
            "\n",
            "batch_num: 202, lr: 0.0014532298082485795, loss: 6.09, smoothed_loss: 8.67, best_loss: 8.67\n",
            " 202/1875 [==>...........................] - ETA: 6s - loss: 6.0929 - accuracy: 0.3424\n",
            "batch_num: 203, lr: 0.0014609259087592363, loss: 6.07, smoothed_loss: 8.61, best_loss: 8.61\n",
            "\n",
            "batch_num: 204, lr: 0.0014686627546325326, loss: 6.05, smoothed_loss: 8.56, best_loss: 8.56\n",
            "\n",
            "batch_num: 205, lr: 0.0014764404622837901, loss: 6.03, smoothed_loss: 8.51, best_loss: 8.51\n",
            "\n",
            "batch_num: 206, lr: 0.0014842594973742962, loss: 6.01, smoothed_loss: 8.46, best_loss: 8.46\n",
            "\n",
            "batch_num: 207, lr: 0.0014921198599040508, loss: 5.99, smoothed_loss: 8.41, best_loss: 8.41\n",
            "\n",
            "batch_num: 208, lr: 0.0015000218991190195, loss: 5.97, smoothed_loss: 8.36, best_loss: 8.36\n",
            "\n",
            "batch_num: 209, lr: 0.001507965731434524, loss: 5.95, smoothed_loss: 8.31, best_loss: 8.31\n",
            "\n",
            "batch_num: 210, lr: 0.00151595170609653, loss: 5.93, smoothed_loss: 8.26, best_loss: 8.26\n",
            "\n",
            "batch_num: 211, lr: 0.001523979939520359, loss: 5.91, smoothed_loss: 8.21, best_loss: 8.21\n",
            "\n",
            "batch_num: 212, lr: 0.001532050664536655, loss: 5.89, smoothed_loss: 8.17, best_loss: 8.17\n",
            "\n",
            "batch_num: 213, lr: 0.0015401641139760613, loss: 5.87, smoothed_loss: 8.12, best_loss: 8.12\n",
            "\n",
            "batch_num: 214, lr: 0.0015483206370845437, loss: 5.85, smoothed_loss: 8.07, best_loss: 8.07\n",
            "\n",
            "batch_num: 215, lr: 0.001556520233862102, loss: 5.83, smoothed_loss: 8.03, best_loss: 8.03\n",
            " 215/1875 [==>...........................] - ETA: 6s - loss: 5.8293 - accuracy: 0.3458\n",
            "batch_num: 216, lr: 0.0015647633699700236, loss: 5.81, smoothed_loss: 7.98, best_loss: 7.98\n",
            "\n",
            "batch_num: 217, lr: 0.0015730500454083085, loss: 5.79, smoothed_loss: 7.94, best_loss: 7.94\n",
            "\n",
            "batch_num: 218, lr: 0.0015813806094229221, loss: 5.77, smoothed_loss: 7.90, best_loss: 7.90\n",
            "\n",
            "batch_num: 219, lr: 0.00158975541125983, loss: 5.76, smoothed_loss: 7.85, best_loss: 7.85\n",
            "\n",
            "batch_num: 220, lr: 0.001598174450919032, loss: 5.74, smoothed_loss: 7.81, best_loss: 7.81\n",
            "\n",
            "batch_num: 221, lr: 0.0016066381940618157, loss: 5.72, smoothed_loss: 7.77, best_loss: 7.77\n",
            "\n",
            "batch_num: 222, lr: 0.001615146640688181, loss: 5.70, smoothed_loss: 7.72, best_loss: 7.72\n",
            "\n",
            "batch_num: 223, lr: 0.0016237001400440931, loss: 5.68, smoothed_loss: 7.68, best_loss: 7.68\n",
            "\n",
            "batch_num: 224, lr: 0.0016322990413755178, loss: 5.66, smoothed_loss: 7.64, best_loss: 7.64\n",
            "\n",
            "batch_num: 225, lr: 0.0016409434610977769, loss: 5.65, smoothed_loss: 7.60, best_loss: 7.60\n",
            "\n",
            "batch_num: 226, lr: 0.001649633515626192, loss: 5.63, smoothed_loss: 7.56, best_loss: 7.56\n",
            "\n",
            "batch_num: 227, lr: 0.0016583697870373726, loss: 5.62, smoothed_loss: 7.52, best_loss: 7.52\n",
            "\n",
            "batch_num: 228, lr: 0.0016671521589159966, loss: 5.60, smoothed_loss: 7.48, best_loss: 7.48\n",
            " 228/1875 [==>...........................] - ETA: 6s - loss: 5.5975 - accuracy: 0.3538\n",
            "batch_num: 229, lr: 0.0016759812133386731, loss: 5.58, smoothed_loss: 7.45, best_loss: 7.45\n",
            "\n",
            "batch_num: 230, lr: 0.0016848568338900805, loss: 5.56, smoothed_loss: 7.41, best_loss: 7.41\n",
            "\n",
            "batch_num: 231, lr: 0.0016937796026468277, loss: 5.55, smoothed_loss: 7.37, best_loss: 7.37\n",
            "\n",
            "batch_num: 232, lr: 0.0017027496360242367, loss: 5.53, smoothed_loss: 7.33, best_loss: 7.33\n",
            "\n",
            "batch_num: 233, lr: 0.0017117670504376292, loss: 5.51, smoothed_loss: 7.30, best_loss: 7.30\n",
            "\n",
            "batch_num: 234, lr: 0.0017208323115482926, loss: 5.50, smoothed_loss: 7.26, best_loss: 7.26\n",
            "\n",
            "batch_num: 235, lr: 0.0017299455357715487, loss: 5.48, smoothed_loss: 7.22, best_loss: 7.22\n",
            "\n",
            "batch_num: 236, lr: 0.001739107072353363, loss: 5.46, smoothed_loss: 7.19, best_loss: 7.19\n",
            "\n",
            "batch_num: 237, lr: 0.0017483170377090573, loss: 5.45, smoothed_loss: 7.15, best_loss: 7.15\n",
            "\n",
            "batch_num: 238, lr: 0.001757575897499919, loss: 5.43, smoothed_loss: 7.12, best_loss: 7.12\n",
            "\n",
            "batch_num: 239, lr: 0.0017668836517259479, loss: 5.42, smoothed_loss: 7.08, best_loss: 7.08\n",
            "\n",
            "batch_num: 240, lr: 0.0017762407660484314, loss: 5.40, smoothed_loss: 7.05, best_loss: 7.05\n",
            " 240/1875 [==>...........................] - ETA: 6s - loss: 5.3994 - accuracy: 0.3589\n",
            "batch_num: 241, lr: 0.0017856474732980132, loss: 5.38, smoothed_loss: 7.02, best_loss: 7.02\n",
            "\n",
            "batch_num: 242, lr: 0.001795104006305337, loss: 5.37, smoothed_loss: 6.98, best_loss: 6.98\n",
            "\n",
            "batch_num: 243, lr: 0.0018046105979010463, loss: 5.36, smoothed_loss: 6.95, best_loss: 6.95\n",
            "\n",
            "batch_num: 244, lr: 0.0018141674809157848, loss: 5.34, smoothed_loss: 6.92, best_loss: 6.92\n",
            "\n",
            "batch_num: 245, lr: 0.0018237750045955181, loss: 5.33, smoothed_loss: 6.89, best_loss: 6.89\n",
            "\n",
            "batch_num: 246, lr: 0.0018334334017708898, loss: 5.31, smoothed_loss: 6.85, best_loss: 6.85\n",
            "\n",
            "batch_num: 247, lr: 0.0018431429052725434, loss: 5.30, smoothed_loss: 6.82, best_loss: 6.82\n",
            "\n",
            "batch_num: 248, lr: 0.0018529039807617664, loss: 5.28, smoothed_loss: 6.79, best_loss: 6.79\n",
            "\n",
            "batch_num: 249, lr: 0.0018627166282385588, loss: 5.27, smoothed_loss: 6.76, best_loss: 6.76\n",
            "\n",
            "batch_num: 250, lr: 0.001872581196948886, loss: 5.26, smoothed_loss: 6.73, best_loss: 6.73\n",
            "\n",
            "batch_num: 251, lr: 0.0018824981525540352, loss: 5.24, smoothed_loss: 6.70, best_loss: 6.70\n",
            "\n",
            "batch_num: 252, lr: 0.0018924674950540066, loss: 5.23, smoothed_loss: 6.67, best_loss: 6.67\n",
            " 252/1875 [===>..........................] - ETA: 6s - loss: 5.2282 - accuracy: 0.3624\n",
            "batch_num: 253, lr: 0.0019024896901100874, loss: 5.21, smoothed_loss: 6.64, best_loss: 6.64\n",
            "\n",
            "batch_num: 254, lr: 0.0019125649705529213, loss: 5.20, smoothed_loss: 6.61, best_loss: 6.61\n",
            "\n",
            "batch_num: 255, lr: 0.001922693569213152, loss: 5.18, smoothed_loss: 6.58, best_loss: 6.58\n",
            "\n",
            "batch_num: 256, lr: 0.0019328758353367448, loss: 5.17, smoothed_loss: 6.56, best_loss: 6.56\n",
            "\n",
            "batch_num: 257, lr: 0.0019431121181696653, loss: 5.16, smoothed_loss: 6.53, best_loss: 6.53\n",
            "\n",
            "batch_num: 258, lr: 0.0019534025341272354, loss: 5.14, smoothed_loss: 6.50, best_loss: 6.50\n",
            "\n",
            "batch_num: 259, lr: 0.0019637474324554205, loss: 5.13, smoothed_loss: 6.47, best_loss: 6.47\n",
            "\n",
            "batch_num: 260, lr: 0.0019741470459848642, loss: 5.12, smoothed_loss: 6.45, best_loss: 6.45\n",
            "\n",
            "batch_num: 261, lr: 0.001984601840376854, loss: 5.11, smoothed_loss: 6.42, best_loss: 6.42\n",
            "\n",
            "batch_num: 262, lr: 0.0019951118156313896, loss: 5.09, smoothed_loss: 6.39, best_loss: 6.39\n",
            "\n",
            "batch_num: 263, lr: 0.0020056776702404022, loss: 5.08, smoothed_loss: 6.37, best_loss: 6.37\n",
            "\n",
            "batch_num: 264, lr: 0.0020162994042038918, loss: 5.07, smoothed_loss: 6.34, best_loss: 6.34\n",
            " 264/1875 [===>..........................] - ETA: 6s - loss: 5.0678 - accuracy: 0.3656\n",
            "batch_num: 265, lr: 0.0020269774831831455, loss: 5.06, smoothed_loss: 6.31, best_loss: 6.31\n",
            "\n",
            "batch_num: 266, lr: 0.0020377119071781635, loss: 5.04, smoothed_loss: 6.29, best_loss: 6.29\n",
            "\n",
            "batch_num: 267, lr: 0.0020485033746808767, loss: 5.03, smoothed_loss: 6.26, best_loss: 6.26\n",
            "\n",
            "batch_num: 268, lr: 0.002059351885691285, loss: 5.02, smoothed_loss: 6.24, best_loss: 6.24\n",
            "\n",
            "batch_num: 269, lr: 0.002070257905870676, loss: 5.01, smoothed_loss: 6.21, best_loss: 6.21\n",
            "\n",
            "batch_num: 270, lr: 0.002081221668049693, loss: 4.99, smoothed_loss: 6.19, best_loss: 6.19\n",
            "\n",
            "batch_num: 271, lr: 0.00209224340505898, loss: 4.98, smoothed_loss: 6.16, best_loss: 6.16\n",
            "\n",
            "batch_num: 272, lr: 0.002103323582559824, loss: 4.97, smoothed_loss: 6.14, best_loss: 6.14\n",
            "\n",
            "batch_num: 273, lr: 0.0021144624333828688, loss: 4.96, smoothed_loss: 6.12, best_loss: 6.12\n",
            "\n",
            "batch_num: 274, lr: 0.002125660190358758, loss: 4.94, smoothed_loss: 6.09, best_loss: 6.09\n",
            "\n",
            "batch_num: 275, lr: 0.002136917319148779, loss: 4.93, smoothed_loss: 6.07, best_loss: 6.07\n",
            " 275/1875 [===>..........................] - ETA: 6s - loss: 4.9333 - accuracy: 0.3686\n",
            "batch_num: 276, lr: 0.0021482340525835752, loss: 4.92, smoothed_loss: 6.05, best_loss: 6.05\n",
            "\n",
            "batch_num: 277, lr: 0.0021596108563244343, loss: 4.91, smoothed_loss: 6.02, best_loss: 6.02\n",
            "\n",
            "batch_num: 278, lr: 0.002171047730371356, loss: 4.90, smoothed_loss: 6.00, best_loss: 6.00\n",
            "\n",
            "batch_num: 279, lr: 0.0021825453732162714, loss: 4.89, smoothed_loss: 5.98, best_loss: 5.98\n",
            "\n",
            "batch_num: 280, lr: 0.0021941037848591805, loss: 4.88, smoothed_loss: 5.96, best_loss: 5.96\n",
            "\n",
            "batch_num: 281, lr: 0.002205723198130727, loss: 4.87, smoothed_loss: 5.94, best_loss: 5.94\n",
            "\n",
            "batch_num: 282, lr: 0.002217404544353485, loss: 4.86, smoothed_loss: 5.91, best_loss: 5.91\n",
            "\n",
            "batch_num: 283, lr: 0.002229147357866168, loss: 4.85, smoothed_loss: 5.89, best_loss: 5.89\n",
            "\n",
            "batch_num: 284, lr: 0.00224095256999135, loss: 4.84, smoothed_loss: 5.87, best_loss: 5.87\n",
            "\n",
            "batch_num: 285, lr: 0.002252820413559675, loss: 4.83, smoothed_loss: 5.85, best_loss: 5.85\n",
            "\n",
            "batch_num: 286, lr: 0.002264750888571143, loss: 4.81, smoothed_loss: 5.83, best_loss: 5.83\n",
            "\n",
            "batch_num: 287, lr: 0.002276744693517685, loss: 4.81, smoothed_loss: 5.81, best_loss: 5.81\n",
            "\n",
            "batch_num: 288, lr: 0.0022888018283993006, loss: 4.79, smoothed_loss: 5.79, best_loss: 5.79\n",
            " 288/1875 [===>..........................] - ETA: 6s - loss: 4.7930 - accuracy: 0.3708\n",
            "batch_num: 289, lr: 0.002300922991707921, loss: 4.78, smoothed_loss: 5.77, best_loss: 5.77\n",
            "\n",
            "batch_num: 290, lr: 0.00231310841627419, loss: 4.77, smoothed_loss: 5.75, best_loss: 5.75\n",
            "\n",
            "batch_num: 291, lr: 0.0023253581020981073, loss: 4.76, smoothed_loss: 5.73, best_loss: 5.73\n",
            "\n",
            "batch_num: 292, lr: 0.002337672980502248, loss: 4.75, smoothed_loss: 5.71, best_loss: 5.71\n",
            "\n",
            "batch_num: 293, lr: 0.0023500528186559677, loss: 4.74, smoothed_loss: 5.69, best_loss: 5.69\n",
            "\n",
            "batch_num: 294, lr: 0.002362498315051198, loss: 4.73, smoothed_loss: 5.67, best_loss: 5.67\n",
            "\n",
            "batch_num: 295, lr: 0.0023750097025185823, loss: 4.72, smoothed_loss: 5.65, best_loss: 5.65\n",
            "\n",
            "batch_num: 296, lr: 0.002387587446719408, loss: 4.71, smoothed_loss: 5.63, best_loss: 5.63\n",
            "\n",
            "batch_num: 297, lr: 0.002400231547653675, loss: 4.70, smoothed_loss: 5.61, best_loss: 5.61\n",
            "\n",
            "batch_num: 298, lr: 0.002412942936643958, loss: 4.69, smoothed_loss: 5.59, best_loss: 5.59\n",
            "\n",
            "batch_num: 299, lr: 0.0024257213808596134, loss: 4.68, smoothed_loss: 5.58, best_loss: 5.58\n",
            " 299/1875 [===>..........................] - ETA: 6s - loss: 4.6777 - accuracy: 0.3731\n",
            "batch_num: 300, lr: 0.002438567578792572, loss: 4.67, smoothed_loss: 5.56, best_loss: 5.56\n",
            "\n",
            "batch_num: 301, lr: 0.0024514817632734776, loss: 4.66, smoothed_loss: 5.54, best_loss: 5.54\n",
            "\n",
            "batch_num: 302, lr: 0.0024644643999636173, loss: 4.65, smoothed_loss: 5.52, best_loss: 5.52\n",
            "\n",
            "batch_num: 303, lr: 0.0024775159545242786, loss: 4.64, smoothed_loss: 5.50, best_loss: 5.50\n",
            "\n",
            "batch_num: 304, lr: 0.0024906364269554615, loss: 4.63, smoothed_loss: 5.49, best_loss: 5.49\n",
            "\n",
            "batch_num: 305, lr: 0.002503826515749097, loss: 4.62, smoothed_loss: 5.47, best_loss: 5.47\n",
            "\n",
            "batch_num: 306, lr: 0.0025170862209051847, loss: 4.61, smoothed_loss: 5.45, best_loss: 5.45\n",
            "\n",
            "batch_num: 307, lr: 0.002530416240915656, loss: 4.60, smoothed_loss: 5.44, best_loss: 5.44\n",
            "\n",
            "batch_num: 308, lr: 0.002543817041441798, loss: 4.60, smoothed_loss: 5.42, best_loss: 5.42\n",
            "\n",
            "batch_num: 309, lr: 0.002557288622483611, loss: 4.59, smoothed_loss: 5.40, best_loss: 5.40\n",
            "\n",
            "batch_num: 310, lr: 0.0025708316825330257, loss: 4.58, smoothed_loss: 5.39, best_loss: 5.39\n",
            " 310/1875 [===>..........................] - ETA: 6s - loss: 4.5771 - accuracy: 0.3729\n",
            "batch_num: 311, lr: 0.002584446221590042, loss: 4.57, smoothed_loss: 5.37, best_loss: 5.37\n",
            "\n",
            "batch_num: 312, lr: 0.002598133170977235, loss: 4.56, smoothed_loss: 5.35, best_loss: 5.35\n",
            "\n",
            "batch_num: 313, lr: 0.0026118922978639603, loss: 4.55, smoothed_loss: 5.34, best_loss: 5.34\n",
            "\n",
            "batch_num: 314, lr: 0.002625724533572793, loss: 4.54, smoothed_loss: 5.32, best_loss: 5.32\n",
            "\n",
            "batch_num: 315, lr: 0.002639629878103733, loss: 4.53, smoothed_loss: 5.30, best_loss: 5.30\n",
            "\n",
            "batch_num: 316, lr: 0.0026536090299487114, loss: 4.52, smoothed_loss: 5.29, best_loss: 5.29\n",
            "\n",
            "batch_num: 317, lr: 0.002667661989107728, loss: 4.51, smoothed_loss: 5.27, best_loss: 5.27\n",
            "\n",
            "batch_num: 318, lr: 0.002681789454072714, loss: 4.50, smoothed_loss: 5.26, best_loss: 5.26\n",
            "\n",
            "batch_num: 319, lr: 0.0026959918905049562, loss: 4.49, smoothed_loss: 5.24, best_loss: 5.24\n",
            "\n",
            "batch_num: 320, lr: 0.002710269298404455, loss: 4.48, smoothed_loss: 5.23, best_loss: 5.23\n",
            "\n",
            "batch_num: 321, lr: 0.0027246226090937853, loss: 4.47, smoothed_loss: 5.21, best_loss: 5.21\n",
            "\n",
            "batch_num: 322, lr: 0.002739051589742303, loss: 4.47, smoothed_loss: 5.20, best_loss: 5.20\n",
            "\n",
            "batch_num: 323, lr: 0.0027535571716725826, loss: 4.46, smoothed_loss: 5.18, best_loss: 5.18\n",
            " 323/1875 [====>.........................] - ETA: 6s - loss: 4.4571 - accuracy: 0.3772\n",
            "batch_num: 324, lr: 0.002768139587715268, loss: 4.45, smoothed_loss: 5.17, best_loss: 5.17\n",
            "\n",
            "batch_num: 325, lr: 0.0027827993035316467, loss: 4.44, smoothed_loss: 5.15, best_loss: 5.15\n",
            "\n",
            "batch_num: 326, lr: 0.002797536551952362, loss: 4.43, smoothed_loss: 5.14, best_loss: 5.14\n",
            "\n",
            "batch_num: 327, lr: 0.0028123517986387014, loss: 4.42, smoothed_loss: 5.12, best_loss: 5.12\n",
            "\n",
            "batch_num: 328, lr: 0.002827245509251952, loss: 4.41, smoothed_loss: 5.11, best_loss: 5.11\n",
            "\n",
            "batch_num: 329, lr: 0.0028422181494534016, loss: 4.40, smoothed_loss: 5.10, best_loss: 5.10\n",
            "\n",
            "batch_num: 330, lr: 0.0028572699520736933, loss: 4.40, smoothed_loss: 5.08, best_loss: 5.08\n",
            "\n",
            "batch_num: 331, lr: 0.0028724016156047583, loss: 4.39, smoothed_loss: 5.07, best_loss: 5.07\n",
            "\n",
            "batch_num: 332, lr: 0.00288761337287724, loss: 4.38, smoothed_loss: 5.05, best_loss: 5.05\n",
            "\n",
            "batch_num: 333, lr: 0.0029029056895524263, loss: 4.37, smoothed_loss: 5.04, best_loss: 5.04\n",
            "\n",
            "batch_num: 334, lr: 0.002918279031291604, loss: 4.36, smoothed_loss: 5.03, best_loss: 5.03\n",
            "\n",
            "batch_num: 335, lr: 0.0029337338637560606, loss: 4.36, smoothed_loss: 5.01, best_loss: 5.01\n",
            " 335/1875 [====>.........................] - ETA: 6s - loss: 4.3564 - accuracy: 0.3799\n",
            "batch_num: 336, lr: 0.0029492704197764397, loss: 4.35, smoothed_loss: 5.00, best_loss: 5.00\n",
            "\n",
            "batch_num: 337, lr: 0.0029648891650140285, loss: 4.34, smoothed_loss: 4.99, best_loss: 4.99\n",
            "\n",
            "batch_num: 338, lr: 0.002980590797960758, loss: 4.33, smoothed_loss: 4.97, best_loss: 4.97\n",
            "\n",
            "batch_num: 339, lr: 0.0029963753186166286, loss: 4.32, smoothed_loss: 4.96, best_loss: 4.96\n",
            "\n",
            "batch_num: 340, lr: 0.0030122436583042145, loss: 4.31, smoothed_loss: 4.95, best_loss: 4.95\n",
            "\n",
            "batch_num: 341, lr: 0.0030281960498541594, loss: 4.31, smoothed_loss: 4.94, best_loss: 4.94\n",
            "\n",
            "batch_num: 342, lr: 0.0030442329589277506, loss: 4.30, smoothed_loss: 4.92, best_loss: 4.92\n",
            "\n",
            "batch_num: 343, lr: 0.003060354618355632, loss: 4.29, smoothed_loss: 4.91, best_loss: 4.91\n",
            "\n",
            "batch_num: 344, lr: 0.003076561726629734, loss: 4.28, smoothed_loss: 4.90, best_loss: 4.90\n",
            "\n",
            "batch_num: 345, lr: 0.0030928547494113445, loss: 4.28, smoothed_loss: 4.88, best_loss: 4.88\n",
            "\n",
            "batch_num: 346, lr: 0.003109233919531107, loss: 4.27, smoothed_loss: 4.87, best_loss: 4.87\n",
            " 346/1875 [====>.........................] - ETA: 6s - loss: 4.2687 - accuracy: 0.3828\n",
            "batch_num: 347, lr: 0.0031256999354809523, loss: 4.26, smoothed_loss: 4.86, best_loss: 4.86\n",
            "\n",
            "batch_num: 348, lr: 0.0031422532629221678, loss: 4.25, smoothed_loss: 4.85, best_loss: 4.85\n",
            "\n",
            "batch_num: 349, lr: 0.0031588939018547535, loss: 4.24, smoothed_loss: 4.84, best_loss: 4.84\n",
            "\n",
            "batch_num: 350, lr: 0.0031756230164319277, loss: 4.24, smoothed_loss: 4.82, best_loss: 4.82\n",
            "\n",
            "batch_num: 351, lr: 0.0031924406066536903, loss: 4.23, smoothed_loss: 4.81, best_loss: 4.81\n",
            "\n",
            "batch_num: 352, lr: 0.0032093471381813288, loss: 4.22, smoothed_loss: 4.80, best_loss: 4.80\n",
            "\n",
            "batch_num: 353, lr: 0.003226343309506774, loss: 4.21, smoothed_loss: 4.79, best_loss: 4.79\n",
            "\n",
            "batch_num: 354, lr: 0.003243429586291313, loss: 4.21, smoothed_loss: 4.78, best_loss: 4.78\n",
            "\n",
            "batch_num: 355, lr: 0.00326060620136559, loss: 4.20, smoothed_loss: 4.77, best_loss: 4.77\n",
            "\n",
            "batch_num: 356, lr: 0.0032778738532215357, loss: 4.19, smoothed_loss: 4.75, best_loss: 4.75\n",
            "\n",
            "batch_num: 357, lr: 0.0032952330075204372, loss: 4.19, smoothed_loss: 4.74, best_loss: 4.74\n",
            " 357/1875 [====>.........................] - ETA: 6s - loss: 4.1851 - accuracy: 0.3865\n",
            "batch_num: 358, lr: 0.0033126838970929384, loss: 4.18, smoothed_loss: 4.73, best_loss: 4.73\n",
            "\n",
            "batch_num: 359, lr: 0.00333022722043097, loss: 4.17, smoothed_loss: 4.72, best_loss: 4.72\n",
            "\n",
            "batch_num: 360, lr: 0.0033478636760264635, loss: 4.17, smoothed_loss: 4.71, best_loss: 4.71\n",
            "\n",
            "batch_num: 361, lr: 0.003365593496710062, loss: 4.16, smoothed_loss: 4.70, best_loss: 4.70\n",
            "\n",
            "batch_num: 362, lr: 0.0033834169153124094, loss: 4.15, smoothed_loss: 4.69, best_loss: 4.69\n",
            "\n",
            "batch_num: 363, lr: 0.003401335095986724, loss: 4.14, smoothed_loss: 4.68, best_loss: 4.68\n",
            "\n",
            "batch_num: 364, lr: 0.0034193480387330055, loss: 4.14, smoothed_loss: 4.67, best_loss: 4.67\n",
            "\n",
            "batch_num: 365, lr: 0.0034374562092125416, loss: 4.13, smoothed_loss: 4.65, best_loss: 4.65\n",
            "\n",
            "batch_num: 366, lr: 0.0034556605387479067, loss: 4.12, smoothed_loss: 4.64, best_loss: 4.64\n",
            "\n",
            "batch_num: 367, lr: 0.003473961027339101, loss: 4.11, smoothed_loss: 4.63, best_loss: 4.63\n",
            "\n",
            "batch_num: 368, lr: 0.0034923586063086987, loss: 4.11, smoothed_loss: 4.62, best_loss: 4.62\n",
            "\n",
            "batch_num: 369, lr: 0.003510853508487344, loss: 4.10, smoothed_loss: 4.61, best_loss: 4.61\n",
            " 369/1875 [====>.........................] - ETA: 6s - loss: 4.1016 - accuracy: 0.3893\n",
            "batch_num: 370, lr: 0.003529446432366967, loss: 4.10, smoothed_loss: 4.60, best_loss: 4.60\n",
            "\n",
            "batch_num: 371, lr: 0.003548137843608856, loss: 4.09, smoothed_loss: 4.59, best_loss: 4.59\n",
            "\n",
            "batch_num: 372, lr: 0.003566928207874298, loss: 4.08, smoothed_loss: 4.58, best_loss: 4.58\n",
            "\n",
            "batch_num: 373, lr: 0.00358581799082458, loss: 4.07, smoothed_loss: 4.57, best_loss: 4.57\n",
            "\n",
            "batch_num: 374, lr: 0.0036048078909516335, loss: 4.07, smoothed_loss: 4.56, best_loss: 4.56\n",
            "\n",
            "batch_num: 375, lr: 0.003623898373916745, loss: 4.06, smoothed_loss: 4.55, best_loss: 4.55\n",
            "\n",
            "batch_num: 376, lr: 0.0036430899053812027, loss: 4.05, smoothed_loss: 4.54, best_loss: 4.54\n",
            "\n",
            "batch_num: 377, lr: 0.003662383183836937, loss: 4.05, smoothed_loss: 4.53, best_loss: 4.53\n",
            "\n",
            "batch_num: 378, lr: 0.0036817784421145916, loss: 4.04, smoothed_loss: 4.52, best_loss: 4.52\n",
            "\n",
            "batch_num: 379, lr: 0.0037012766115367413, loss: 4.03, smoothed_loss: 4.51, best_loss: 4.51\n",
            "\n",
            "batch_num: 380, lr: 0.0037208779249340296, loss: 4.03, smoothed_loss: 4.50, best_loss: 4.50\n",
            "\n",
            "batch_num: 381, lr: 0.0037405830807983875, loss: 4.02, smoothed_loss: 4.49, best_loss: 4.49\n",
            "\n",
            "batch_num: 382, lr: 0.0037603925447911024, loss: 4.01, smoothed_loss: 4.48, best_loss: 4.48\n",
            " 382/1875 [=====>........................] - ETA: 6s - loss: 4.0149 - accuracy: 0.3917\n",
            "batch_num: 383, lr: 0.003780307015404105, loss: 4.01, smoothed_loss: 4.47, best_loss: 4.47\n",
            "\n",
            "batch_num: 384, lr: 0.0038003267254680395, loss: 4.00, smoothed_loss: 4.46, best_loss: 4.46\n",
            "\n",
            "batch_num: 385, lr: 0.0038204528391361237, loss: 3.99, smoothed_loss: 4.45, best_loss: 4.45\n",
            "\n",
            "batch_num: 386, lr: 0.003840685123577714, loss: 3.99, smoothed_loss: 4.45, best_loss: 4.45\n",
            "\n",
            "batch_num: 387, lr: 0.0038610247429460287, loss: 3.98, smoothed_loss: 4.44, best_loss: 4.44\n",
            "\n",
            "batch_num: 388, lr: 0.003881472162902355, loss: 3.98, smoothed_loss: 4.43, best_loss: 4.43\n",
            "\n",
            "batch_num: 389, lr: 0.0039020278491079807, loss: 3.97, smoothed_loss: 4.42, best_loss: 4.42\n",
            "\n",
            "batch_num: 390, lr: 0.003922692500054836, loss: 3.96, smoothed_loss: 4.41, best_loss: 4.41\n",
            "\n",
            "batch_num: 391, lr: 0.003943466115742922, loss: 3.96, smoothed_loss: 4.40, best_loss: 4.40\n",
            " 391/1875 [=====>........................] - ETA: 6s - loss: 3.9585 - accuracy: 0.3928\n",
            "batch_num: 392, lr: 0.003964350093156099, loss: 3.95, smoothed_loss: 4.39, best_loss: 4.39\n",
            "\n",
            "batch_num: 393, lr: 0.003985344897955656, loss: 3.95, smoothed_loss: 4.38, best_loss: 4.38\n",
            "\n",
            "batch_num: 394, lr: 0.004006450530141592, loss: 3.94, smoothed_loss: 4.37, best_loss: 4.37\n",
            "\n",
            "batch_num: 395, lr: 0.004027667921036482, loss: 3.93, smoothed_loss: 4.36, best_loss: 4.36\n",
            "\n",
            "batch_num: 396, lr: 0.0040489980019629, loss: 3.93, smoothed_loss: 4.36, best_loss: 4.36\n",
            "\n",
            "batch_num: 397, lr: 0.004070440772920847, loss: 3.92, smoothed_loss: 4.35, best_loss: 4.35\n",
            "\n",
            "batch_num: 398, lr: 0.004091997165232897, loss: 3.91, smoothed_loss: 4.34, best_loss: 4.34\n",
            "\n",
            "batch_num: 399, lr: 0.004113667644560337, loss: 3.91, smoothed_loss: 4.33, best_loss: 4.33\n",
            "\n",
            "batch_num: 400, lr: 0.004135452676564455, loss: 3.90, smoothed_loss: 4.32, best_loss: 4.32\n",
            "\n",
            "batch_num: 401, lr: 0.004157353658229113, loss: 3.90, smoothed_loss: 4.31, best_loss: 4.31\n",
            "\n",
            "batch_num: 402, lr: 0.0041793701238930225, loss: 3.89, smoothed_loss: 4.30, best_loss: 4.30\n",
            " 402/1875 [=====>........................] - ETA: 6s - loss: 3.8938 - accuracy: 0.3946\n",
            "batch_num: 403, lr: 0.004201503470540047, loss: 3.89, smoothed_loss: 4.30, best_loss: 4.30\n",
            "\n",
            "batch_num: 404, lr: 0.004223753698170185, loss: 3.88, smoothed_loss: 4.29, best_loss: 4.29\n",
            "\n",
            "batch_num: 405, lr: 0.0042461222037673, loss: 3.87, smoothed_loss: 4.28, best_loss: 4.28\n",
            "\n",
            "batch_num: 406, lr: 0.00426860898733139, loss: 3.87, smoothed_loss: 4.27, best_loss: 4.27\n",
            "\n",
            "batch_num: 407, lr: 0.004291214980185032, loss: 3.86, smoothed_loss: 4.26, best_loss: 4.26\n",
            "\n",
            "batch_num: 408, lr: 0.004313940182328224, loss: 3.86, smoothed_loss: 4.25, best_loss: 4.25\n",
            "\n",
            "batch_num: 409, lr: 0.004336785990744829, loss: 3.85, smoothed_loss: 4.25, best_loss: 4.25\n",
            "\n",
            "batch_num: 410, lr: 0.0043597533367574215, loss: 3.85, smoothed_loss: 4.24, best_loss: 4.24\n",
            "\n",
            "batch_num: 411, lr: 0.004382841754704714, loss: 3.84, smoothed_loss: 4.23, best_loss: 4.23\n",
            "\n",
            "batch_num: 412, lr: 0.004406052641570568, loss: 3.83, smoothed_loss: 4.22, best_loss: 4.22\n",
            "\n",
            "batch_num: 413, lr: 0.004429385997354984, loss: 3.83, smoothed_loss: 4.21, best_loss: 4.21\n",
            " 413/1875 [=====>........................] - ETA: 6s - loss: 3.8285 - accuracy: 0.3981\n",
            "batch_num: 414, lr: 0.004452843684703112, loss: 3.82, smoothed_loss: 4.21, best_loss: 4.21\n",
            "\n",
            "batch_num: 415, lr: 0.0044764247722923756, loss: 3.82, smoothed_loss: 4.20, best_loss: 4.20\n",
            "\n",
            "batch_num: 416, lr: 0.004500131588429213, loss: 3.81, smoothed_loss: 4.19, best_loss: 4.19\n",
            "\n",
            "batch_num: 417, lr: 0.004523963201791048, loss: 3.81, smoothed_loss: 4.18, best_loss: 4.18\n",
            "\n",
            "batch_num: 418, lr: 0.004547921475023031, loss: 3.80, smoothed_loss: 4.18, best_loss: 4.18\n",
            "\n",
            "batch_num: 419, lr: 0.004572006408125162, loss: 3.80, smoothed_loss: 4.17, best_loss: 4.17\n",
            "\n",
            "batch_num: 420, lr: 0.004596218932420015, loss: 3.79, smoothed_loss: 4.16, best_loss: 4.16\n",
            "\n",
            "batch_num: 421, lr: 0.0046205599792301655, loss: 3.78, smoothed_loss: 4.15, best_loss: 4.15\n",
            "\n",
            "batch_num: 422, lr: 0.0046450295485556126, loss: 3.78, smoothed_loss: 4.15, best_loss: 4.15\n",
            "\n",
            "batch_num: 423, lr: 0.0046696290373802185, loss: 3.77, smoothed_loss: 4.14, best_loss: 4.14\n",
            "\n",
            "batch_num: 424, lr: 0.004694358445703983, loss: 3.77, smoothed_loss: 4.13, best_loss: 4.13\n",
            " 424/1875 [=====>........................] - ETA: 6s - loss: 3.7680 - accuracy: 0.4010\n",
            "batch_num: 425, lr: 0.004719219170510769, loss: 3.76, smoothed_loss: 4.12, best_loss: 4.12\n",
            "\n",
            "batch_num: 426, lr: 0.004744211211800575, loss: 3.76, smoothed_loss: 4.12, best_loss: 4.12\n",
            "\n",
            "batch_num: 427, lr: 0.004769335966557264, loss: 3.75, smoothed_loss: 4.11, best_loss: 4.11\n",
            "\n",
            "batch_num: 428, lr: 0.004794593434780836, loss: 3.75, smoothed_loss: 4.10, best_loss: 4.10\n",
            "\n",
            "batch_num: 429, lr: 0.0048199850134551525, loss: 3.75, smoothed_loss: 4.09, best_loss: 4.09\n",
            "\n",
            "batch_num: 430, lr: 0.0048455107025802135, loss: 3.74, smoothed_loss: 4.09, best_loss: 4.09\n",
            "\n",
            "batch_num: 431, lr: 0.004871171433478594, loss: 3.74, smoothed_loss: 4.08, best_loss: 4.08\n",
            "\n",
            "batch_num: 432, lr: 0.004896968603134155, loss: 3.73, smoothed_loss: 4.07, best_loss: 4.07\n",
            "\n",
            "batch_num: 433, lr: 0.004922902211546898, loss: 3.73, smoothed_loss: 4.07, best_loss: 4.07\n",
            "\n",
            "batch_num: 434, lr: 0.004948972724378109, loss: 3.72, smoothed_loss: 4.06, best_loss: 4.06\n",
            "\n",
            "batch_num: 435, lr: 0.004975182004272938, loss: 3.72, smoothed_loss: 4.05, best_loss: 4.05\n",
            "\n",
            "batch_num: 436, lr: 0.005001529585570097, loss: 3.72, smoothed_loss: 4.05, best_loss: 4.05\n",
            " 436/1875 [=====>........................] - ETA: 6s - loss: 3.7168 - accuracy: 0.4007\n",
            "batch_num: 437, lr: 0.0050280168652534485, loss: 3.71, smoothed_loss: 4.04, best_loss: 4.04\n",
            "\n",
            "batch_num: 438, lr: 0.00505464430898428, loss: 3.71, smoothed_loss: 4.03, best_loss: 4.03\n",
            "\n",
            "batch_num: 439, lr: 0.005081412848085165, loss: 3.70, smoothed_loss: 4.03, best_loss: 4.03\n",
            "\n",
            "batch_num: 440, lr: 0.005108323413878679, loss: 3.70, smoothed_loss: 4.02, best_loss: 4.02\n",
            "\n",
            "batch_num: 441, lr: 0.005135376006364822, loss: 3.69, smoothed_loss: 4.01, best_loss: 4.01\n",
            "\n",
            "batch_num: 442, lr: 0.005162572022527456, loss: 3.69, smoothed_loss: 4.01, best_loss: 4.01\n",
            "\n",
            "batch_num: 443, lr: 0.005189912393689156, loss: 3.68, smoothed_loss: 4.00, best_loss: 4.00\n",
            "\n",
            "batch_num: 444, lr: 0.00521739711984992, loss: 3.68, smoothed_loss: 3.99, best_loss: 3.99\n",
            "\n",
            "batch_num: 445, lr: 0.005245027597993612, loss: 3.67, smoothed_loss: 3.99, best_loss: 3.99\n",
            " 445/1875 [======>.......................] - ETA: 6s - loss: 3.6747 - accuracy: 0.4016\n",
            "batch_num: 446, lr: 0.005272804759442806, loss: 3.67, smoothed_loss: 3.98, best_loss: 3.98\n",
            "\n",
            "batch_num: 447, lr: 0.005300728604197502, loss: 3.67, smoothed_loss: 3.97, best_loss: 3.97\n",
            "\n",
            "batch_num: 448, lr: 0.005328800063580275, loss: 3.66, smoothed_loss: 3.97, best_loss: 3.97\n",
            "\n",
            "batch_num: 449, lr: 0.0053570205345749855, loss: 3.66, smoothed_loss: 3.96, best_loss: 3.96\n",
            "\n",
            "batch_num: 450, lr: 0.005385390482842922, loss: 3.65, smoothed_loss: 3.96, best_loss: 3.96\n",
            "\n",
            "batch_num: 451, lr: 0.005413910839706659, loss: 3.65, smoothed_loss: 3.95, best_loss: 3.95\n",
            "\n",
            "batch_num: 452, lr: 0.005442582070827484, loss: 3.64, smoothed_loss: 3.94, best_loss: 3.94\n",
            "\n",
            "batch_num: 453, lr: 0.005471404641866684, loss: 3.64, smoothed_loss: 3.94, best_loss: 3.94\n",
            "\n",
            "batch_num: 454, lr: 0.005500380415469408, loss: 3.64, smoothed_loss: 3.93, best_loss: 3.93\n",
            "\n",
            "batch_num: 455, lr: 0.005529509391635656, loss: 3.63, smoothed_loss: 3.93, best_loss: 3.93\n",
            "\n",
            "batch_num: 456, lr: 0.005558792967349291, loss: 3.63, smoothed_loss: 3.92, best_loss: 3.92\n",
            " 456/1875 [======>.......................] - ETA: 6s - loss: 3.6270 - accuracy: 0.4010\n",
            "batch_num: 457, lr: 0.005588231608271599, loss: 3.62, smoothed_loss: 3.91, best_loss: 3.91\n",
            "\n",
            "batch_num: 458, lr: 0.005617825780063868, loss: 3.62, smoothed_loss: 3.91, best_loss: 3.91\n",
            "\n",
            "batch_num: 459, lr: 0.005647576879709959, loss: 3.62, smoothed_loss: 3.90, best_loss: 3.90\n",
            "\n",
            "batch_num: 460, lr: 0.0056774853728711605, loss: 3.61, smoothed_loss: 3.90, best_loss: 3.90\n",
            "\n",
            "batch_num: 461, lr: 0.005707552656531334, loss: 3.61, smoothed_loss: 3.89, best_loss: 3.89\n",
            "\n",
            "batch_num: 462, lr: 0.005737778730690479, loss: 3.60, smoothed_loss: 3.88, best_loss: 3.88\n",
            "\n",
            "batch_num: 463, lr: 0.0057681649923324585, loss: 3.60, smoothed_loss: 3.88, best_loss: 3.88\n",
            "\n",
            "batch_num: 464, lr: 0.005798712372779846, loss: 3.59, smoothed_loss: 3.87, best_loss: 3.87\n",
            "\n",
            "batch_num: 465, lr: 0.00582942133769393, loss: 3.59, smoothed_loss: 3.87, best_loss: 3.87\n",
            "\n",
            "batch_num: 466, lr: 0.0058602928183972836, loss: 3.59, smoothed_loss: 3.86, best_loss: 3.86\n",
            "\n",
            "batch_num: 467, lr: 0.00589132821187377, loss: 3.58, smoothed_loss: 3.86, best_loss: 3.86\n",
            " 467/1875 [======>.......................] - ETA: 6s - loss: 3.5828 - accuracy: 0.4005\n",
            "batch_num: 468, lr: 0.005922527518123388, loss: 3.58, smoothed_loss: 3.85, best_loss: 3.85\n",
            "\n",
            "batch_num: 469, lr: 0.005953892134130001, loss: 3.58, smoothed_loss: 3.85, best_loss: 3.85\n",
            "\n",
            "batch_num: 470, lr: 0.005985422991216183, loss: 3.58, smoothed_loss: 3.84, best_loss: 3.84\n",
            "\n",
            "batch_num: 471, lr: 0.006017121020704508, loss: 3.58, smoothed_loss: 3.83, best_loss: 3.83\n",
            "\n",
            "batch_num: 472, lr: 0.006048986688256264, loss: 3.57, smoothed_loss: 3.83, best_loss: 3.83\n",
            "\n",
            "batch_num: 473, lr: 0.006081020925194025, loss: 3.57, smoothed_loss: 3.82, best_loss: 3.82\n",
            "\n",
            "batch_num: 474, lr: 0.006113225128501654, loss: 3.56, smoothed_loss: 3.82, best_loss: 3.82\n",
            "\n",
            "batch_num: 475, lr: 0.006145599763840437, loss: 3.56, smoothed_loss: 3.81, best_loss: 3.81\n",
            "\n",
            "batch_num: 476, lr: 0.0061781457625329494, loss: 3.56, smoothed_loss: 3.81, best_loss: 3.81\n",
            "\n",
            "batch_num: 477, lr: 0.006210864055901766, loss: 3.55, smoothed_loss: 3.80, best_loss: 3.80\n",
            " 477/1875 [======>.......................] - ETA: 6s - loss: 3.5521 - accuracy: 0.3992\n",
            "batch_num: 478, lr: 0.006243756040930748, loss: 3.55, smoothed_loss: 3.80, best_loss: 3.80\n",
            "\n",
            "batch_num: 479, lr: 0.006276821717619896, loss: 3.54, smoothed_loss: 3.79, best_loss: 3.79\n",
            "\n",
            "batch_num: 480, lr: 0.006310062948614359, loss: 3.54, smoothed_loss: 3.79, best_loss: 3.79\n",
            "\n",
            "batch_num: 481, lr: 0.006343479733914137, loss: 3.54, smoothed_loss: 3.78, best_loss: 3.78\n",
            "\n",
            "batch_num: 482, lr: 0.006377073936164379, loss: 3.54, smoothed_loss: 3.78, best_loss: 3.78\n",
            "\n",
            "batch_num: 483, lr: 0.006410846021026373, loss: 3.53, smoothed_loss: 3.77, best_loss: 3.77\n",
            "\n",
            "batch_num: 484, lr: 0.0064447964541614056, loss: 3.53, smoothed_loss: 3.77, best_loss: 3.77\n",
            "\n",
            "batch_num: 485, lr: 0.006478927098214626, loss: 3.53, smoothed_loss: 3.76, best_loss: 3.76\n",
            "\n",
            "batch_num: 486, lr: 0.0065132384188473225, loss: 3.52, smoothed_loss: 3.76, best_loss: 3.76\n",
            "\n",
            "batch_num: 487, lr: 0.006547731347382069, loss: 3.52, smoothed_loss: 3.75, best_loss: 3.75\n",
            "\n",
            "batch_num: 488, lr: 0.006582407280802727, loss: 3.52, smoothed_loss: 3.75, best_loss: 3.75\n",
            " 488/1875 [======>.......................] - ETA: 6s - loss: 3.5158 - accuracy: 0.3967\n",
            "batch_num: 489, lr: 0.006617266684770584, loss: 3.51, smoothed_loss: 3.74, best_loss: 3.74\n",
            "\n",
            "batch_num: 490, lr: 0.006652310490608215, loss: 3.51, smoothed_loss: 3.74, best_loss: 3.74\n",
            "\n",
            "batch_num: 491, lr: 0.006687540095299482, loss: 3.51, smoothed_loss: 3.74, best_loss: 3.74\n",
            "\n",
            "batch_num: 492, lr: 0.0067229559645056725, loss: 3.50, smoothed_loss: 3.73, best_loss: 3.73\n",
            "\n",
            "batch_num: 493, lr: 0.006758559960871935, loss: 3.50, smoothed_loss: 3.73, best_loss: 3.73\n",
            "\n",
            "batch_num: 494, lr: 0.00679435208439827, loss: 3.49, smoothed_loss: 3.72, best_loss: 3.72\n",
            "\n",
            "batch_num: 495, lr: 0.006830333732068539, loss: 3.49, smoothed_loss: 3.72, best_loss: 3.72\n",
            "\n",
            "batch_num: 496, lr: 0.0068665058352053165, loss: 3.49, smoothed_loss: 3.71, best_loss: 3.71\n",
            "\n",
            "batch_num: 497, lr: 0.006902869790792465, loss: 3.48, smoothed_loss: 3.71, best_loss: 3.71\n",
            "\n",
            "batch_num: 498, lr: 0.006939426530152559, loss: 3.48, smoothed_loss: 3.70, best_loss: 3.70\n",
            "\n",
            "batch_num: 499, lr: 0.006976176518946886, loss: 3.47, smoothed_loss: 3.70, best_loss: 3.70\n",
            " 499/1875 [======>.......................] - ETA: 6s - loss: 3.4745 - accuracy: 0.3969\n",
            "batch_num: 500, lr: 0.0070131211541593075, loss: 3.47, smoothed_loss: 3.69, best_loss: 3.69\n",
            "\n",
            "batch_num: 501, lr: 0.007050261367112398, loss: 3.47, smoothed_loss: 3.69, best_loss: 3.69\n",
            "\n",
            "batch_num: 502, lr: 0.00708759855479002, loss: 3.47, smoothed_loss: 3.68, best_loss: 3.68\n",
            "\n",
            "batch_num: 503, lr: 0.00712513318285346, loss: 3.46, smoothed_loss: 3.68, best_loss: 3.68\n",
            "\n",
            "batch_num: 504, lr: 0.007162866648286581, loss: 3.46, smoothed_loss: 3.68, best_loss: 3.68\n",
            "\n",
            "batch_num: 505, lr: 0.007200799882411957, loss: 3.46, smoothed_loss: 3.67, best_loss: 3.67\n",
            "\n",
            "batch_num: 506, lr: 0.0072389342822134495, loss: 3.45, smoothed_loss: 3.67, best_loss: 3.67\n",
            "\n",
            "batch_num: 507, lr: 0.007277270313352346, loss: 3.45, smoothed_loss: 3.66, best_loss: 3.66\n",
            "\n",
            "batch_num: 508, lr: 0.007315809838473797, loss: 3.45, smoothed_loss: 3.66, best_loss: 3.66\n",
            "\n",
            "batch_num: 509, lr: 0.007354552857577801, loss: 3.44, smoothed_loss: 3.65, best_loss: 3.65\n",
            " 509/1875 [=======>......................] - ETA: 6s - loss: 3.4429 - accuracy: 0.3948\n",
            "batch_num: 510, lr: 0.007393501233309507, loss: 3.44, smoothed_loss: 3.65, best_loss: 3.65\n",
            "\n",
            "batch_num: 511, lr: 0.007432656362652779, loss: 3.44, smoothed_loss: 3.65, best_loss: 3.65\n",
            "\n",
            "batch_num: 512, lr: 0.0074720182456076145, loss: 3.43, smoothed_loss: 3.64, best_loss: 3.64\n",
            "\n",
            "batch_num: 513, lr: 0.007511588744819164, loss: 3.43, smoothed_loss: 3.64, best_loss: 3.64\n",
            "\n",
            "batch_num: 514, lr: 0.0075513687916100025, loss: 3.43, smoothed_loss: 3.63, best_loss: 3.63\n",
            "\n",
            "batch_num: 515, lr: 0.007591359782963991, loss: 3.42, smoothed_loss: 3.63, best_loss: 3.63\n",
            "\n",
            "batch_num: 516, lr: 0.0076315621845424175, loss: 3.42, smoothed_loss: 3.62, best_loss: 3.62\n",
            "\n",
            "batch_num: 517, lr: 0.007671977858990431, loss: 3.42, smoothed_loss: 3.62, best_loss: 3.62\n",
            "\n",
            "batch_num: 518, lr: 0.007712607271969318, loss: 3.42, smoothed_loss: 3.62, best_loss: 3.62\n",
            "\n",
            "batch_num: 519, lr: 0.007753452286124229, loss: 3.41, smoothed_loss: 3.61, best_loss: 3.61\n",
            "\n",
            "batch_num: 520, lr: 0.007794512901455164, loss: 3.41, smoothed_loss: 3.61, best_loss: 3.61\n",
            "\n",
            "batch_num: 521, lr: 0.007835791446268559, loss: 3.41, smoothed_loss: 3.60, best_loss: 3.60\n",
            " 521/1875 [=======>......................] - ETA: 5s - loss: 3.4078 - accuracy: 0.3929\n",
            "batch_num: 522, lr: 0.0078772883862257, loss: 3.40, smoothed_loss: 3.60, best_loss: 3.60\n",
            "\n",
            "batch_num: 523, lr: 0.007919005118310452, loss: 3.40, smoothed_loss: 3.60, best_loss: 3.60\n",
            "\n",
            "batch_num: 524, lr: 0.007960943505167961, loss: 3.40, smoothed_loss: 3.59, best_loss: 3.59\n",
            "\n",
            "batch_num: 525, lr: 0.008003102615475655, loss: 3.40, smoothed_loss: 3.59, best_loss: 3.59\n",
            "\n",
            "batch_num: 526, lr: 0.00804548617452383, loss: 3.39, smoothed_loss: 3.58, best_loss: 3.58\n",
            "\n",
            "batch_num: 527, lr: 0.008088093250989914, loss: 3.39, smoothed_loss: 3.58, best_loss: 3.58\n",
            "\n",
            "batch_num: 528, lr: 0.008130926638841629, loss: 3.39, smoothed_loss: 3.58, best_loss: 3.58\n",
            "\n",
            "batch_num: 529, lr: 0.00817398726940155, loss: 3.39, smoothed_loss: 3.57, best_loss: 3.57\n",
            "\n",
            "batch_num: 530, lr: 0.008217275142669678, loss: 3.38, smoothed_loss: 3.57, best_loss: 3.57\n",
            "\n",
            "batch_num: 531, lr: 0.00826079212129116, loss: 3.38, smoothed_loss: 3.57, best_loss: 3.57\n",
            "\n",
            "batch_num: 532, lr: 0.008304540067911148, loss: 3.38, smoothed_loss: 3.56, best_loss: 3.56\n",
            " 532/1875 [=======>......................] - ETA: 5s - loss: 3.3771 - accuracy: 0.3907\n",
            "batch_num: 533, lr: 0.008348519913852215, loss: 3.37, smoothed_loss: 3.56, best_loss: 3.56\n",
            "\n",
            "batch_num: 534, lr: 0.00839273165911436, loss: 3.37, smoothed_loss: 3.55, best_loss: 3.55\n",
            "\n",
            "batch_num: 535, lr: 0.00843717809766531, loss: 3.37, smoothed_loss: 3.55, best_loss: 3.55\n",
            "\n",
            "batch_num: 536, lr: 0.008481860160827637, loss: 3.36, smoothed_loss: 3.55, best_loss: 3.55\n",
            "\n",
            "batch_num: 537, lr: 0.008526778779923916, loss: 3.36, smoothed_loss: 3.54, best_loss: 3.54\n",
            "\n",
            "batch_num: 538, lr: 0.008571934886276722, loss: 3.36, smoothed_loss: 3.54, best_loss: 3.54\n",
            "\n",
            "batch_num: 539, lr: 0.008617330342531204, loss: 3.36, smoothed_loss: 3.54, best_loss: 3.54\n",
            "\n",
            "batch_num: 540, lr: 0.008662967011332512, loss: 3.35, smoothed_loss: 3.53, best_loss: 3.53\n",
            "\n",
            "batch_num: 541, lr: 0.00870884396135807, loss: 3.35, smoothed_loss: 3.53, best_loss: 3.53\n",
            "\n",
            "batch_num: 542, lr: 0.008754964917898178, loss: 3.35, smoothed_loss: 3.52, best_loss: 3.52\n",
            "\n",
            "batch_num: 543, lr: 0.008801329880952835, loss: 3.35, smoothed_loss: 3.52, best_loss: 3.52\n",
            "\n",
            "batch_num: 544, lr: 0.008847939781844616, loss: 3.34, smoothed_loss: 3.52, best_loss: 3.52\n",
            " 544/1875 [=======>......................] - ETA: 5s - loss: 3.3441 - accuracy: 0.3896\n",
            "batch_num: 545, lr: 0.008894797414541245, loss: 3.34, smoothed_loss: 3.51, best_loss: 3.51\n",
            "\n",
            "batch_num: 546, lr: 0.00894190277904272, loss: 3.34, smoothed_loss: 3.51, best_loss: 3.51\n",
            "\n",
            "batch_num: 547, lr: 0.008989257737994194, loss: 3.34, smoothed_loss: 3.51, best_loss: 3.51\n",
            "\n",
            "batch_num: 548, lr: 0.009036863222718239, loss: 3.34, smoothed_loss: 3.50, best_loss: 3.50\n",
            "\n",
            "batch_num: 549, lr: 0.009084721095860004, loss: 3.33, smoothed_loss: 3.50, best_loss: 3.50\n",
            "\n",
            "batch_num: 550, lr: 0.009132832288742065, loss: 3.33, smoothed_loss: 3.50, best_loss: 3.50\n",
            "\n",
            "batch_num: 551, lr: 0.009181197732686996, loss: 3.33, smoothed_loss: 3.49, best_loss: 3.49\n",
            "\n",
            "batch_num: 552, lr: 0.009229820221662521, loss: 3.33, smoothed_loss: 3.49, best_loss: 3.49\n",
            "\n",
            "batch_num: 553, lr: 0.00927869975566864, loss: 3.36, smoothed_loss: 3.49, best_loss: 3.49\n",
            "\n",
            "batch_num: 554, lr: 0.009327838197350502, loss: 3.36, smoothed_loss: 3.49, best_loss: 3.49\n",
            " 554/1875 [=======>......................] - ETA: 5s - loss: 3.3580 - accuracy: 0.3873\n",
            "batch_num: 555, lr: 0.009377236478030682, loss: 3.91, smoothed_loss: 3.49, best_loss: 3.49\n",
            "\n",
            "batch_num: 556, lr: 0.009426897391676903, loss: 4.45, smoothed_loss: 3.51, best_loss: 3.49\n",
            "\n",
            "batch_num: 557, lr: 0.009476820006966591, loss: 4.50, smoothed_loss: 3.53, best_loss: 3.49\n",
            "\n",
            "batch_num: 558, lr: 0.009527008049190044, loss: 5.54, smoothed_loss: 3.57, best_loss: 3.49\n",
            "\n",
            "batch_num: 559, lr: 0.009577461518347263, loss: 11.10, smoothed_loss: 3.72, best_loss: 3.49\n",
            "\n",
            "batch_num: 560, lr: 0.009628182277083397, loss: 12.04, smoothed_loss: 3.89, best_loss: 3.49\n",
            "\n",
            "batch_num: 561, lr: 0.00967917125672102, loss: 16.11, smoothed_loss: 4.13, best_loss: 3.49\n",
            "\n",
            "batch_num: 562, lr: 0.009730430319905281, loss: 17.87, smoothed_loss: 4.41, best_loss: 3.49\n",
            "\n",
            "batch_num: 563, lr: 0.00978196132928133, loss: 24.74, smoothed_loss: 4.82, best_loss: 3.49\n",
            "\n",
            "batch_num: 564, lr: 0.009833765216171741, loss: 88.54, smoothed_loss: 6.49, best_loss: 3.49\n",
            "\n",
            "Loss is too big now, cancelling training.\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: nan - accuracy: 0.1830 - val_loss: nan - val_accuracy: 0.0980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9zD5hKkHdbl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "c71bb4e3-c083-4b1d-e308-5c6356c4a7e3"
      },
      "source": [
        "lr_finder.plot_lr()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVbF_rdroQxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}